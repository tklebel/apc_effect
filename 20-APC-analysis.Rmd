---
title: "Relationship between OA publishing, APCs and IF"
author: "Thomas Klebel"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, dpi = 300)

# setup -----
Sys.setenv(SPARK_HOME = "/usr/hdp/current/spark2-client")
library(sparklyr)
library(tidyverse)
library(arrow)
library(patchwork)
source(here::here("R/helpers.R"))

message("Connecting to spark...")

config <- spark_config()
config$spark.executor.cores <- 5 # this should always stay the same
config$spark.executor.instances <- 8 # this can go up to 27, depending on RAM
config$spark.executor.memory <- "25G"
sc <- spark_connect(master = "yarn-client", config = config,
                    app_name = "OA_APCs")
message("Connection to Spark successful!")

spark_read_parquet(
  sc, "works", "/user/tklebel/apc_paper/papers_with_concepts.parquet",
  memory = TRUE
)
works <- tbl(sc, "works")

message("Successfully read all datasets!")

theme_set(theme_bw())
```


# Distribution across topics
How many papers do we have, which also have a topic?
```{r}
works %>% 
  filter(author_position == "first", !is.na(display_name)) %>% 
  sdf_nrow()
```
This is only slightly lower than the total number of papers we have.



Which topics are represented in our sample?
```{r}
frac_concept_papers <- works %>% 
  filter(author_position == "first") %>% 
  group_by(display_name) %>% 
  summarise(frac_papers = sum(concept_frac)) %>% 
  arrange(desc(frac_papers)) %>% 
  collect()
```


```{r concept-overview}
frac_concept_papers %>% 
  drop_na() %>% 
  ggplot(aes(frac_papers, fct_reorder(display_name, frac_papers))) +
  geom_col(width = .7) +
  scale_x_continuous(labels = scales::comma) +
  labs(y = NULL, x = "Fractional papers")
```

```{r}
mean_apcs <- works %>%
  # first get rid of duplicates from concepts
  distinct(id, author_position, work_frac, APC_in_dollar, University, country,
           publication_year, PP_top10) %>% 
  group_by(University, publication_year, country, PP_top10) %>%
  # compute the average APC using fractional authorships as weights
  mutate(sum_frac = sum(work_frac)) %>%
  group_by(University, publication_year, country, PP_top10, sum_frac,
           author_position) %>%
  summarise(mean_apc = sum(work_frac * APC_in_dollar) / sum_frac)

mean_apcs_local <- mean_apcs %>%
  collect()
```

```{r apc-first-last, fig.width=8, fig.height=4}
# plot for 2019
apc_16_19 <- mean_apcs_local %>%
  filter(publication_year > 2015 & publication_year < 2020)

labels <- apc_16_19 %>%
  group_by(author_position) %>%
  summarise(cor = cor(mean_apc, PP_top10, use = "pairwise.complete")) %>%
  mutate(cor = glue::glue("r = {format(cor, nsmall = 2, digits = 2)}"))

apc_16_19 %>%
  ggplot(aes(PP_top10, mean_apc)) +
  geom_point(aes(colour = sum_frac),
             alpha = .5) +
  geom_smooth() +
  facet_wrap(vars(author_position)) +
  geom_text(data = labels, aes(label = cor, x = .25, y = 2250)) +
  scale_colour_viridis_c(trans = "sqrt") +
  labs(caption = "2016-2019")
```


```{r}
mean_apcs_by_concept <- works %>%
  group_by(University, publication_year, country, PP_top10, display_name) %>%
  # compute the average APC using fractional authorships as weights
  mutate(sum_frac = sum(work_frac)) %>%
  group_by(University, publication_year, country, PP_top10, sum_frac,
           author_position, display_name) %>%
  summarise(mean_apc = sum(work_frac * APC_in_dollar) / sum_frac)

apcs_by_concept_local <- mean_apcs_by_concept %>%
  collect()
```


```{r apc-concepts, fig.height=20, fig.width=6}
# plot for 2016-2019
apc_concept_16_19 <- apcs_by_concept_local %>%
  filter(publication_year > 2015 & publication_year < 2020,
         !is.na(display_name))

labels <- apc_concept_16_19 %>%
  group_by(author_position, display_name) %>%
  summarise(cor = cor(mean_apc, PP_top10, use = "pairwise.complete.obs")) %>%
  mutate(cor = glue::glue("r = {format(cor, nsmall = 2, digits = 0)}"))

apc_concept_16_19 %>%
  ggplot(aes(PP_top10, mean_apc)) +
  geom_point(aes(colour = sum_frac),
             alpha = .7) +
  geom_smooth() +
  facet_grid(cols = vars(author_position),
             rows = vars(str_wrap(display_name, 10))) +
  geom_text(data = labels, aes(label = cor, x = .3, y = 4000)) +
  scale_colour_viridis_c(trans = "log10") +
  labs(caption = "2016-2019")
```


```{r}
spark_disconnect(sc)
```

