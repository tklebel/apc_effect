---
title: "Understanding log/log marginal effects"
author: "Thomas Klebel"
date: '2022-06-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(marginaleffects)
library(brms)

theme_set(theme_bw())
```

We start out by simulating an exponential x.
```{r}
n <- 100
set.seed(298374)

df <- tibble(x = rnorm(n, mean = 2, sd = .8) %>% exp())
```

This is the kind of distribution that I work with as an independent variable.
```{r}
df %>% 
  ggplot(aes(x)) +
  geom_density()
```

If we take the log of it, it looks almost gaussian.

```{r}
df %>% 
  mutate(x = log(x)) %>% 
  ggplot(aes(x)) +
  geom_density()
```

Now we simulate an y that depends on x, using the model we have in mind: a 
log-log model.

```{r}
e <- rnorm(n, sd = .2)

df <- df %>% 
  mutate(y = exp(5 + .4*log(x) + e))
```

This is of course also an exponential distribution.

```{r}
df %>% 
  ggplot(aes(y)) +
  geom_density()
```
The relationship is not linear at all.
```{r}
p <- df %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth()
p
```
But if we take both logs, it is linear.
```{r}
p + 
  scale_x_continuous(trans = "log") +
  scale_y_continuous(trans = "log") +
  geom_smooth()
```

# Modelling the log log
Let us model it now
```{r}
lm(log(y) ~ log(x), data = df)
```

This gets us the parameter back.


```{r}
df <- df %>% 
  mutate(log_x = log(x))
m1 <- brm(y ~ log_x, family = lognormal(), data = df,
          chains = 1, iter = 200, warmup = 100)
```
```{r}
m1
```
```{r}
marginaleffects(m1, newdata = datagrid(log_x = c(1, 2, 3)),
                type = "link")
```

This works as intended: if we stay on the link scale, we can interpret as 
proportions.

Just to understand what the comparisons function is doing. This seems to lead
to roughly the same result, although with slight numerical difference.
```{r}
comp <- comparisons(m1, newdata = datagrid(log_x = 1:3), transform_pre = "ratio",
            variables = list(log_x = log(101/100)), type = "response")
comp
```

However, according to the document from Benoit, this is the more correct version
than directly interpreting the coefficient (see the file linked here: 
https://darrendahly.github.io/post/loglog/).

```{r}
exp(log(101/100) * 0.4417)
```

## Visualising draws of the comparison ourselves
```{r}
comp %>% 
  marginaleffects::posteriordraws() %>% 
  group_by(log_x) %>% 
  summarise(mean = mean(draw))
```

```{r}
comp %>% 
  marginaleffects::posteriordraws() %>% 
  ggplot(aes(draw, factor(log_x))) +
  tidybayes::stat_halfeye()
```



Two remaining questions:
- what happens in case of hurdle model?
- what happens in case of mixture of hurdle models?


# Simulate hurdle
```{r}
df <- df %>% 
  mutate(z = runif(n, min = -3, max = 0),
         y_hurdle = case_when(
           rbinom(n, size = 1, prob = rethinking::inv_logit(z)) > 0 ~ 0,
           TRUE ~ y))
```


```{r}
df %>% 
  ggplot(aes(y_hurdle)) +
  geom_density()
```

```{r}
hm <- brm(bf(y_hurdle ~ log_x,
             hu ~ z),
          family = hurdle_lognormal(), data = df,
          chains = 1, iter = 300, warmup = 150)
```


```{r}
hm
```

```{r}
marginaleffects(hm, newdata = datagrid(log_x = 1:3,
                                       z = 0))
```


```{r}
comparisons(
  hm, 
  newdata = datagrid(log_x = c(1:3),
                     z = 0),
  type = "link"
)
```
Apparently without the mixture we really can get the link response. So in the
mixture model, this might be an issue?

```{r}
comparisons(
  hm, 
  variables = list(log_x = log(150/100)),
  newdata = datagrid(log_x = c(1:3),
                     z = 0),
  type = "link"
)
```
This is stable across all x, as expected. It just now shows the change in y for 
a 50% change ine x.
No, it does not.
this would be
```{r}
exp(log(150/100) * 0.4274205 )
```
= 18.9%, according to the file from benoit that was linked here: https://darrendahly.github.io/post/loglog/


What do these changes again look like on the response? Not sure if the following
would be correct.

```{r}
comparisons(
  hm, 
  variables = list(log_x = log(101/100)),
  newdata = datagrid(log_x = c(1:3),
                     z = 0),
  type = "response"
)
```

Interestingly, this is about the same as we get from using marginaleffects,
but on a different scale.
```{r}
marginaleffects(hm, newdata = datagrid(log_x = 1:3,
                                       z = 0))
```

Given the warning message, this might not be taking the hurdle component into
account. The similarity in values might again be due to us being close to zero
in terms of our exposure.

## Calulating contrast manually
Can we retrieve and calculate this ourselves from predictions?

```{r}
contrast <- predictions(hm, newdata = datagrid(log_x = c(2, log(exp(2)*1.01)),
                                   z = 0)) %>% 
  posteriordraws() %>% 
  mutate(x = factor(log_x, labels = c("base", "step"))) %>% 
  pivot_wider(-c(predicted, conf.low, conf.high, log_x, rowid), 
              names_from = x, values_from = draw) %>% 
  mutate(contrast = step / base)
contrast


contrast %>% 
  summarise(mean = mean(contrast))
```

yes, we can!! 

This is exactly the same as when following the rule from Benoit:

```{r}
fixef(hm)
exp(log(101/(100) * 0.4284994))
```

It should be interpreted as "1% change in x leads to .42% change in y. This is 
exactly as we specified in the simulation at the start (with some error attached
due to simulating).

We can also visualise it

```{r}
contrast %>% 
  ggplot(aes(contrast)) +
  tidybayes::stat_halfeye()
```

This should allow us to compute the ratio contrast regardless of the mixture,
since we can always predict, and predict for such a contrast.


To validate, we need to simulate the mixture.
